{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chargeback_rpt.db_postgres_command as db_command\n",
    "import chargeback_rpt.vm_data_utility as vm_util\n",
    "import chargeback_rpt.vm_data_validator as vx\n",
    "import chargeback_rpt.email_notifier as x_mail\n",
    "import chargeback_rpt.file_directory_manager as fd_mn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For run script .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datasource_key='nimble_vol'\n",
    "datasource_key='primera_vol'\n",
    "\n",
    "run_py=False\n",
    "fix_mmyy=True\n",
    "is_only_check_data=False\n",
    "\n",
    "x_datenow=datetime.datetime.now()\n",
    "month_x=x_datenow.strftime('%m')\n",
    "year_x=x_datenow.strftime('%Y')\n",
    "\n",
    "if fix_mmyy:\n",
    " month_param='10'\n",
    " year_param='2021'\n",
    "else:\n",
    " month_param=month_x\n",
    " year_param=year_x\n",
    "    \n",
    "    \n",
    "print(f\"Current month-year : {month_param}-{year_param}\")\n",
    "\n",
    "if  run_py:\n",
    "\n",
    "    press_y='n'\n",
    "    ok=False\n",
    "    \n",
    "    if len(sys.argv) > 1 :\n",
    "        if sys.argv[1]=='0'  and sys.argv[2]=='0'  and  (sys.argv[3]=='1' or sys.argv[3]=='2'):\n",
    "         month_param=month_x\n",
    "         year_param=year_x \n",
    "         if sys.argv[3]=='1':\n",
    "            datasource_key='nimble_vol'\n",
    "         elif sys.argv[3]=='2':\n",
    "            datasource_key='primera_vol'   \n",
    "         \n",
    "         is_only_check_data=bool(int(sys.argv[4]))\n",
    "            \n",
    "         ok=True   \n",
    "#          if ok:\n",
    "#           f = open(f'test_run_ok_{datasource_key}.txt', 'a') \n",
    "#           f.write(f\"ok run {datasource_key}\")\n",
    "#           f.close()\n",
    "            \n",
    "        else:\n",
    "         print(\"The system are required 3 arguments as the following like this.\")   \n",
    "         print(\"0 0 1\")  \n",
    "         print(\"0 0 2\")  \n",
    "         raise Exception(\"Specify arguments orderly such as  0 as first , 0 as second ,nimble_vol or primera_vol as third \")   \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Enter HPE storage to import data.\")\n",
    "        print(\"1 is Nimble\")\n",
    "        print(\"2 is Primera\")\n",
    "        datasource_type =int(input(\"Enter storage model , press 1  or  2: \"))\n",
    "        if datasource_type in [1,2]:\n",
    "            if datasource_type==1:\n",
    "               datasource_key='nimble_vol'\n",
    "            else:\n",
    "               datasource_key='primera_vol'\n",
    "        else:\n",
    "             raise Exception(\"Invalid storage model , allow only press the following choices\\n 1 = Nimble \\n 2 = Primera\")   \n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"Enter month and year . If you want current month and year.Press Enter \")\n",
    "        month_param = int(input(\"Enter the month (1-12) : \") or month_x)\n",
    "        year_param = int(input(\"Enter the year(such as 2021,2022,2023...) : \") or year_x)\n",
    "        print(\"============================================================================\")\n",
    "        print(f\"Do import {datasource_key.title()} for month={month_param} and year={year_param}\")\n",
    "    \n",
    "        press_y=input(f\"Press y=True and any keys=False : \") \n",
    "        if press_y.lower()=='y':\n",
    "         ok=True\n",
    "\n",
    "\n",
    "    if ok==True:\n",
    "        try:\n",
    "         report_dt= datetime.datetime(int(year_param), int(month_param), 1)\n",
    "         print(report_dt)   \n",
    "         month_param=report_dt.strftime('%m')\n",
    "         year_param=report_dt.strftime('%Y')   \n",
    "\n",
    "        except Exception as ex:\n",
    "         print(ex)   \n",
    "         raise ex\n",
    "    else:\n",
    "         quit()\n",
    "            \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check in job_type table\n",
    "\n",
    "if is_only_check_data==False:\n",
    " job_type = 6 if  datasource_key=='nimble_vol' else 7 \n",
    "else:\n",
    " job_type = 10 if  datasource_key=='nimble_vol' else 11\n",
    " \n",
    "\n",
    "t_id=vm_util.creating_transaction(job_type,month_param,year_param)\n",
    "print(f\"ETL Transaction ID: {t_id}\")\n",
    "\n",
    "list_error=[]\n",
    "print(list_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  datasource_key=='nimble_vol':\n",
    " comment_col='comment_nimble'\n",
    " covert_from_to_gb='byte'\n",
    " used_size_col='size_used_gb_nimble'   \n",
    " vol_master_col='volume_name_nimble'   \n",
    " job_type=6\n",
    " tb_key=\"nimble_table\"\n",
    " hpe_col_key='nimble'   \n",
    "\n",
    "elif datasource_key=='primera_vol':\n",
    " comment_col='comment_primera'\n",
    " covert_from_to_gb='mb'\n",
    " used_size_col='size_used_gb_primera'  \n",
    " vol_master_col='volume_name_primera'   \n",
    " job_type=7\n",
    " tb_key=\"primera_table\"\n",
    " hpe_col_key='premira'   \n",
    "\n",
    "path_key='import_path'\n",
    "\n",
    "center_col='cost_center'\n",
    "system_col='system_name'\n",
    "created_date_col='created_date'\n",
    "\n",
    "cs_cols_toCheck=[comment_col,center_col,system_col]\n",
    "\n",
    "hpe_ok=False\n",
    "vol_filepath=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move  to util\n",
    "def check_error_point_etl(tran_id):\n",
    "  print(list_error)\n",
    "  if True in list_error:\n",
    "\n",
    "        \n",
    "    etl_files=[]\n",
    "    if  vol_filepath is not None and os.path.exists(vol_filepath):\n",
    "     etl_files.append([hpe_ok,vol_filepath])   \n",
    "    \n",
    "    if len(etl_files)>0 and (hpe_ok==False) :\n",
    "     vm_util.finished_etl_folder(t_id,hpe_col_key,False,etl_files)  \n",
    "        \n",
    "    vm_util.collect_error_to_sent_mail(tran_id) \n",
    "    \n",
    "    print(\"ETL occured error\") \n",
    "    \n",
    "    raise Exception(\"Program is teminated and check error from email and log_error.txt\")\n",
    "    \n",
    "  list_error.clear()\n",
    "  print(list_error)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datetime_format = vm_util.get_config_value(\"datetime_format\", t_id)\n",
    "    print(datetime_format)\n",
    "    \n",
    "except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    \n",
    "check_error_point_etl(t_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"get root folder and source file name for ready to do ETL\")\n",
    "\n",
    "try: \n",
    " \n",
    " # load root path\n",
    " import_path=vm_util.get_config_value(path_key,t_id)\n",
    " existPath,is_error=vx.check_existing_filepath(import_path,t_id)\n",
    " list_error.append(is_error)   \n",
    " if existPath:\n",
    "   print(\"Get Path of all source files: \",import_path)   \n",
    "\n",
    " # load datasource data    \n",
    " sr_vol,is_error=vx.check_source_name_file(datasource_key,t_id)\n",
    " list_error.append(is_error) \n",
    " print(sr_vol)\n",
    " \n",
    " # load datafsource file paht\n",
    " vol_filepath=f\"{import_path}\\\\{sr_vol['source_prefix']}.{sr_vol['source_type']}\"\n",
    " a,is_error=vx.check_existing_filepath(vol_filepath,t_id)\n",
    " list_error.append(is_error)    \n",
    " print(\"Get Path of volume file: \",vol_filepath,' is ',a)   \n",
    "\n",
    " # Select columns from source to load\n",
    " print(\"Selec required columns from source file for dataframe\")   \n",
    " df_vol_mappingFields=vm_util.get_active_datafield(sr_vol['id'])\n",
    " print(df_vol_mappingFields[['column_table_name','field_source_name','is_not_null','is_cost_column']])\n",
    " \n",
    " vol_cols=df_vol_mappingFields['field_source_name'].tolist()   \n",
    " print(vol_cols)\n",
    "    \n",
    " \n",
    " \n",
    " # load data field   \n",
    "\n",
    "\n",
    "except Exception as ex:\n",
    " list_error.append(True)  \n",
    "   \n",
    "    \n",
    "check_error_point_etl(t_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load volume file\")\n",
    "\n",
    "if os.path.isfile(vol_filepath) and os.path.getsize(vol_filepath) > 0:\n",
    " df_vol= pd.read_csv(vol_filepath,usecols=vol_cols)\n",
    "\n",
    " if df_vol.empty==False:\n",
    " \n",
    "     print(\"Mapping external columnns from source file to  interal fields on system\")\n",
    "     df_vol,err=vx.map_fields_ds_to_cols_df(df_vol,df_vol_mappingFields,vol_filepath,t_id)\n",
    "\n",
    "     print(df_vol.info())\n",
    "     print(df_vol)\n",
    "    \n",
    " else:\n",
    "    \n",
    "     if is_only_check_data==True:\n",
    "          delete_resultList= fd_mn.delele_file(vol_filepath)\n",
    "     else:\n",
    "          vm_util.finished_etl_folder(t_id,hpe_col_key,True,[[True,vol_filepath]])    \n",
    "            \n",
    "     updated_rows=vm_util.created_transaction(t_id)\n",
    "     print(f\"completed ETL with No volumne on {hpe_col_key.title()} for  month={month_param} and year={year_param}\")   \n",
    "     exit()   \n",
    "    \n",
    "    \n",
    "else:\n",
    "\n",
    "  try:\n",
    "    \n",
    "     if is_only_check_data==True:\n",
    "          delete_resultList= fd_mn.delele_file(vol_filepath)\n",
    "     else:\n",
    "          vm_util.finished_etl_folder(t_id,hpe_col_key,True,[[True,vol_filepath]]) \n",
    "        \n",
    "     updated_rows=vm_util.created_transaction(t_id)\n",
    "     print(f\"completed ETL with No volumne on {hpe_col_key.title()} for  month={month_param} and year={year_param}\")   \n",
    "     exit()      \n",
    "    \n",
    "  except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    print(ex)\n",
    "    \n",
    "\n",
    "#df_vol\n",
    "check_error_point_etl(t_id)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extract and Transform Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extract Cost Center and System Name and Created Date\")\n",
    "df_vol[center_col]=df_vol[comment_col].apply(vm_util.split_comment_to_each_value,args=(center_col,)) \n",
    "df_vol[system_col]=df_vol[comment_col].apply(vm_util.split_comment_to_each_value,args=(system_col,))\n",
    "df_vol[created_date_col]=df_vol[comment_col].apply(vm_util.split_comment_to_each_value,args=(created_date_col,))\n",
    "\n",
    "\n",
    "#df_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check invalid  data after extracting comment\")\n",
    "cols_na_comment,df_na_comment=vx.find_NaN(df_vol,cs_cols_toCheck)\n",
    "\n",
    "print(cols_na_comment)\n",
    "print(df_na_comment)\n",
    "\n",
    "if df_na_comment.shape[0]>0:\n",
    "    \n",
    " str_error = '</br>Summarize rows that found any null or incorrect format string in <b><u>comment column</u></b></br>'\n",
    " df_na_comment= df_na_comment.fillna(\"\")\n",
    " str_error += df_na_comment.to_html(index=False)\n",
    " vm_util.add_error_to_database(16, str_error,t_id)\n",
    " list_error.append(True)   \n",
    "\n",
    "\n",
    "print(df_vol.info())\n",
    "print(df_vol)\n",
    "\n",
    "\n",
    "\n",
    "check_error_point_etl(t_id)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert string date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "lastDay_val=datetime.date(int(year_param),int(month_param),calendar.monthrange(int(year_param), int(month_param))[1])\n",
    "print(lastDay_val)\n",
    "print(\"Validate CreatedDate\")\n",
    "\n",
    "df_vol=vx.find_incorrrect_format_for_input_datetime(df_vol,created_date_col,vol_master_col,datetime_format,t_id)\n",
    "if df_vol is None:\n",
    " list_error.append(True)\n",
    " print(list_error)\n",
    "else:    \n",
    " df_vol=vx.find_invalid_x_date_greater_last_date_for_input_datetime(df_vol,created_date_col,lastDay_val,vol_master_col,t_id)\n",
    " if df_vol is None:\n",
    "  list_error.append(True) \n",
    "  print(list_error)\n",
    " else:\n",
    "   print(f\"all created_date > {lastDay_val}\")   \n",
    "   print( df_vol[[vol_master_col,created_date_col]] ) \n",
    "    \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert size unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert size unit\")\n",
    "df_vol[used_size_col]=df_vol[used_size_col].apply(vm_util.convert_Xb_unit_Gb,args=(covert_from_to_gb,))\n",
    "#df_vol  \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vol.info())\n",
    "print(df_vol)\n",
    "#df_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Addtional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform addtoinal data\")\n",
    "try:\n",
    "     \n",
    "\n",
    " df_vol['month']=month_param\n",
    " df_vol['year']=year_param\n",
    " df_vol['import_date']=x_datenow\n",
    "\n",
    "\n",
    " df_vol['transaction_id']= t_id\n",
    "\n",
    " df_vol['system_name']=df_vol['system_name'].apply(vx.replace_x_character)\n",
    " df_vol['cost_center']=df_vol['cost_center'].apply(vx.replace_x_character)   \n",
    "\n",
    "\n",
    "except Exception as err:\n",
    "  list_error.append(True)\n",
    "  error_message= str(err)\n",
    "  print(error_message)   \n",
    "  vm_util.add_error_to_database(14,error_message,t_id)  \n",
    " \n",
    "check_error_point_etl(t_id)\n",
    "hpe_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vol.info())\n",
    "print(df_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End up with  Completed ETL in only Checking Data mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_only_check_data==True:\n",
    "    try: \n",
    "\n",
    "        delete_resultList= fd_mn.delele_file(vol_filepath)   \n",
    "        updated_rows=vm_util.created_transaction(t_id)\n",
    "        print(\"completed ETL in only Checking Data mode\")\n",
    "        \n",
    "        exit()\n",
    "        \n",
    "        \n",
    "    except Exception as ex:\n",
    "        list_error.append(True)\n",
    "        print(ex)\n",
    "        raise Exception(\"Program is teminated and check error from email and log_error.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"save  volumn dataframe to database\")\n",
    "table_name,listCols_report,is_error=vx.listCols_report_table(tb_key,t_id)\n",
    "list_error.append(is_error)\n",
    "\n",
    "\n",
    "print(f'list all columns in {table_name} table: ',len(listCols_report))\n",
    "print(listCols_report)\n",
    "\n",
    "\n",
    "listCols_dfx=df_vol.columns.tolist()\n",
    "print(f'list all columns in {table_name} dataframe: ',len(listCols_dfx))\n",
    "print(listCols_dfx)\n",
    "\n",
    "\n",
    "check_columns_2=vm_util.verify_existing_columns(listCols_dfx,listCols_report)\n",
    "\n",
    "print(check_columns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save data to database\")\n",
    "if check_columns_2  is None:\n",
    "   df_vol=df_vol[listCols_report]\n",
    "   print(df_vol.info())\n",
    "   print(df_vol.head())\n",
    "   rslt=db_command.add_data_values(db_command.get_postgres_conn(),df_vol,table_name)\n",
    "   #print(rslt)\n",
    "    \n",
    "else:\n",
    "   list_error.append(True)\n",
    "   error_message=check_columns_2\n",
    "   vm_util.add_error_to_database(10,error_message,t_id)\n",
    "   print(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check New CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Update new cost center\")\n",
    "\n",
    "try :\n",
    " cost_centerList = df_vol['cost_center'].unique()\n",
    " df_new_cc=vm_util.add_new_cost_center(cost_centerList,f'{hpe_col_key.title()}-ETL',t_id)\n",
    " print(df_new_cc)\n",
    "except Exception as ex:\n",
    " list_error.append(True)   \n",
    " print(ex)   \n",
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    etl_files=[[hpe_ok,vol_filepath]]\n",
    "    if hpe_ok==True :\n",
    "     vm_util.finished_etl_folder(t_id,hpe_col_key,True,etl_files)    \n",
    "     updated_rows=vm_util.created_transaction(t_id)\n",
    "     print(\"completed ETL\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    print(ex)\n",
    "    \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
