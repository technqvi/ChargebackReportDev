{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chargeback_rpt.db_postgres_command as db_command\n",
    "import chargeback_rpt.veeam_data_extractor as rpt_veeam\n",
    "import chargeback_rpt.vm_data_utility as vm_util\n",
    "import chargeback_rpt.vm_data_validator as vx\n",
    "import chargeback_rpt.vm_data_charger as vc\n",
    "import chargeback_rpt.email_notifier as x_mail\n",
    "import chargeback_rpt.file_directory_manager as fd_mn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pytz import timezone\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "\n",
    "\n",
    "import json\n",
    "import sys \n",
    "\n",
    "# update 23/12/22 filter exclude \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For run script .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_py=False\n",
    "\n",
    "print(\"Update 23/12/22\")\n",
    "print(\"1.remove vCLS-\")\n",
    "print(\"2.change if/else statement : List VM-Off contain terminated_date\")\n",
    "\n",
    "is_only_check_data=False  # Check data=True , ETL=False\n",
    "use_backup=False   # Use Veeam=True, Disable veem=False\n",
    "\n",
    "fix_mmyy=False  #fix_mmyy=True  for test_only_jupyternotebook\n",
    "\n",
    "veeam_ok=False\n",
    "vdisk_ok=False\n",
    "vinfo_ok=False\n",
    "\n",
    "veeam_filepath=None\n",
    "disk_filepath=None\n",
    "info_filepath=None\n",
    "\n",
    "# disable veeam  \n",
    "if is_only_check_data==True:\n",
    " use_backup=False  # Use Veeam=True, Disable veem=False\n",
    " veeam_ok=True\n",
    "\n",
    "\n",
    "x_datenow=datetime.datetime.now()\n",
    "month_x=x_datenow.strftime('%m')\n",
    "year_x=x_datenow.strftime('%Y')\n",
    "\n",
    "if fix_mmyy:\n",
    " month_param='09'\n",
    " year_param='2021'\n",
    "else:\n",
    " month_param=month_x\n",
    " year_param=year_x\n",
    "\n",
    "print(f\"Current month-year : {month_param}-{year_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv[1]=month  , sys.argv[2]=year ,  sys.argv[3]=is_check_data such as 0=check_only , 1=actual run\n",
    "\n",
    "if  run_py:\n",
    "\n",
    "    press_y='n'\n",
    "    ok=False\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        if sys.argv[1]=='0'  and sys.argv[2]=='0':\n",
    "            \n",
    "         month_param=month_x\n",
    "         year_param=year_x \n",
    "         ok=True   \n",
    "         is_only_check_data=bool(int(sys.argv[3]))\n",
    "            \n",
    "        else:\n",
    "         raise Exception(\"Specify 0 0 as arguments only\")   \n",
    "    else:\n",
    "        print(\"If you want current month and year.Press Enter \")\n",
    "        month_param = int(input(\"Enter the month (1-12) : \") or month_x)\n",
    "        year_param = int(input(\"Enter the year(such as 2021,2022,2023...) : \") or year_x)\n",
    "        print(f\"Do for month={month_param} and year={year_param}\")\n",
    "        press_y=input(f\"Press y=True and n=False : \") \n",
    "        if press_y.lower()=='y':\n",
    "         ok=True\n",
    "\n",
    "\n",
    "    if ok==True:\n",
    "        try:\n",
    "         report_dt= datetime.datetime(int(year_param), int(month_param), 1)\n",
    "         print(report_dt)   \n",
    "         month_param=report_dt.strftime('%m')\n",
    "         year_param=report_dt.strftime('%Y')   \n",
    "\n",
    "    #      n = len(sys.argv) \n",
    "    #      print(\"Total arguments passed:\", n) \n",
    "    #      for  param in  sys.argv:\n",
    "    #        print(param)\n",
    "\n",
    "        except Exception as ex:\n",
    "         print(ex)   \n",
    "         raise ex\n",
    "    else:\n",
    "         quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_only_check_data==False:\n",
    " t_id=vm_util.creating_transaction(1,month_param,year_param)\n",
    " \n",
    "else:\n",
    "  t_id=vm_util.creating_transaction(8,month_param,year_param)   \n",
    "    \n",
    "print(f\"ETL Transaction ID: {t_id}  and OnlyCheckData={is_only_check_data}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_error=[]\n",
    "print(list_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move  to util\n",
    "def check_error_point_etl(tran_id):\n",
    "  print(list_error)\n",
    "  if True in list_error:\n",
    "        \n",
    "    etl_files=[]\n",
    "# disable veeam    \n",
    "#     if  veeam_filepath is not None and os.path.exists(veeam_filepath):\n",
    "#      etl_files.append([veeam_ok,veeam_filepath])\n",
    "\n",
    "    if  (disk_filepath is not None )  and (os.path.exists(disk_filepath)):\n",
    "     etl_files.append([vdisk_ok,disk_filepath])  \n",
    "    if (info_filepath is not None) and (os.path.exists(info_filepath)):\n",
    "     etl_files.append([vinfo_ok,info_filepath])     \n",
    "    \n",
    "    if len(etl_files)>0 and ( (veeam_ok==False) or (vdisk_ok==False)  or (vinfo_ok==False) ):\n",
    "       vm_util.finished_etl_folder(t_id,vm_col_key,False,etl_files)  \n",
    "    \n",
    "    vm_util.collect_error_to_sent_mail(tran_id)  \n",
    "    print(\"ETL occured error\") \n",
    "    \n",
    "    raise Exception(\"Program is teminated and check error from email and log_error.txt\")\n",
    "    \n",
    "  list_error.clear()\n",
    "  print(list_error)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_tb_key=\"vm_table\"\n",
    "vm_col_key='vm'\n",
    "\n",
    "try:\n",
    " unknow_val=vm_util.get_config_value(\"unknow\",t_id)\n",
    " datetime_format = vm_util.get_config_value(\"datetime_format\", t_id)\n",
    "except:\n",
    " list_error.append(True)   \n",
    " print(error_message)   \n",
    "    \n",
    "\n",
    "update_additional_cost_key='email_link_update_additional_cost'\n",
    "all_additional_cost_key='email_link_all_additional_cost'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify all source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"get root folder\")\n",
    "\n",
    "key_path='import_path'\n",
    "import_path=vm_util.get_value_by_key(key_path)\n",
    "print(import_path)\n",
    "\n",
    "if import_path is not None:\n",
    " \n",
    " data_import_path=import_path['value']\n",
    " existPath,is_error=vx.check_existing_filepath(data_import_path,t_id)\n",
    " list_error.append(is_error)   \n",
    " if existPath:\n",
    "  print(\"Get Path of all source files: \",data_import_path)\n",
    "else:\n",
    " list_error.append(True)   \n",
    " error_message= f'no key:{key_path} in key column in config_value table'\n",
    " print(error_message)   \n",
    " vm_util.add_error_to_database(3,error_message,t_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"get source file name for ready to do ETL\")\n",
    "\n",
    "sr_veeam_ds,is_error=vx.check_source_name_file('veeamone',t_id)\n",
    "print(sr_veeam_ds)\n",
    "list_error.append(is_error)\n",
    "\n",
    "sr_disk_ds,is_error=vx.check_source_name_file('rvtools_disk',t_id)\n",
    "print(sr_disk_ds)\n",
    "list_error.append(is_error)\n",
    "\n",
    "sr_datasource,is_error=vx.check_source_name_file('rvtools_info',t_id)\n",
    "print(sr_datasource)\n",
    "list_error.append(is_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"check file path for doing ETL\")\n",
    "def check_existing_file_path(path,is_onefile_folder) :\n",
    "    \n",
    " if is_onefile_folder==True:\n",
    "   try:  # new version\n",
    "    result=vm_util.extract_single_file_as_specified_name_in_folder(path,t_id) \n",
    "    print(f'{path} = {result}')\n",
    "   except Exception as error:\n",
    "    list_error.append(True)    \n",
    " else:  # check version1\n",
    "    is_existing,is_error=vx.check_existing_filepath(path,t_id)  \n",
    "    print(f'{path} = {is_existing}')\n",
    "    list_error.append(is_error)\n",
    "    \n",
    "print(\"=========================================================================\") \n",
    "if is_only_check_data==False:  \n",
    "    print(\"check Path : VeeamOne\")\n",
    "    print(sr_veeam_ds)\n",
    "    veeam_filepath=f\"{data_import_path}\\\\{sr_veeam_ds['source_prefix']}.{sr_veeam_ds['source_type']}\"\n",
    "    # disable veeam\n",
    "    #check_existing_file_path(veeam_filepath,sr_veeam_ds['is_onefile_folder'] )\n",
    "print(\"=========================================================================\")   \n",
    "\n",
    "print(\"check Path : VMDisk\")\n",
    "print(sr_disk_ds)\n",
    "disk_filepath=f\"{data_import_path}\\\\{sr_disk_ds['source_prefix']}.{sr_disk_ds['source_type']}\"\n",
    "check_existing_file_path(disk_filepath,sr_disk_ds['is_onefile_folder'] )\n",
    "print(\"=========================================================================\")  \n",
    "\n",
    "print(\"check Path : VMInfo\")\n",
    "print(sr_datasource)\n",
    "info_filepath=f\"{data_import_path}\\\\{sr_datasource['source_prefix']}.{sr_datasource['source_type']}\"\n",
    "check_existing_file_path(info_filepath,sr_datasource['is_onefile_folder'] )\n",
    "print(\"=========================================================================\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Veeam-One Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_backup==True:\n",
    "    try: \n",
    "     print(\"load veeam-one files\")\n",
    "     df_veeam_mappingFields=vm_util.get_active_datafield(sr_veeam_ds['id'])\n",
    "     print(df_veeam_mappingFields)\n",
    "\n",
    "     df_veeam=rpt_veeam.extract_veeam_report_data(veeam_filepath,df_veeam_mappingFields)\n",
    "     if df_veeam is not None:\n",
    "      print(df_veeam.info())      \n",
    "      print(df_veeam.head(10))\n",
    "      print(df_veeam.isna().sum())  \n",
    "\n",
    "      dict_veeam_mapplingCols=dict(zip(df_veeam_mappingFields['field_source_name'],df_veeam_mappingFields['column_table_name']))\n",
    "      print(dict_veeam_mapplingCols)\n",
    "      df_veeam.rename(columns=dict_veeam_mapplingCols,inplace=True) \n",
    "      print(df_veeam.head(20))\n",
    "\n",
    "\n",
    "     if df_veeam is None:\n",
    "        raise  Exception(\"No any backup policy in Veeam\")\n",
    "     else:\n",
    "        list_error.append(False)  \n",
    "\n",
    "\n",
    "      #df_veeam.to_excel('veeam_temp.xlsx',index=False)\n",
    "     #=========================================================\n",
    "    except Exception as error:\n",
    "     list_error.append(True)   \n",
    "     vm_util.add_error_to_database(6,str(error),t_id)\n",
    "     print('error is ',str(error))\n",
    "\n",
    "# disable veeam\n",
    "else:\n",
    "    print(f\"No need veeam file path due to only check data  OR disabled veeam\")\n",
    "#     df_veeam=pd.DataFrame(data=list())\n",
    "#     print(df_veeam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)\n",
    "veeam_ok=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load vinfo and vdisk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load vinfo and vdisk files\")\n",
    "\n",
    "df_vDisk,is_error=vx.load_source_file(disk_filepath,sr_disk_ds['source_keyname'],t_id)\n",
    "list_error.append(is_error)\n",
    "\n",
    "df_vInfo,is_error=vx.load_source_file(info_filepath,sr_datasource['source_keyname'],t_id)\n",
    "list_error.append(is_error)\n",
    " \n",
    "check_error_point_etl(t_id) \n",
    "vdisk_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vDisk.head(5))\n",
    "print(df_vInfo[['VM','CreatedDate']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selec required columns from dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selec required columns from dataframe\")\n",
    "\n",
    "print(\"select field in disk info\")\n",
    "df_disk_mappingFields=vm_util.get_active_datafield(sr_disk_ds['id'])\n",
    "print(\"==================vDisk mappingFields==================================\")\n",
    "print(df_disk_mappingFields[['column_table_name','field_source_name','is_additional','datasource_id']])\n",
    "df_vDisk,err=vx.select_colunm_df_fieldname_ds(df_vDisk,df_disk_mappingFields['field_source_name'].tolist(),disk_filepath,t_id)\n",
    "list_error.append(err)\n",
    "\n",
    "print(\"=====================================================================================================\")\n",
    "\n",
    "\n",
    "print(\"select field in vm info\")\n",
    "df_mappingFields=vm_util.get_active_datafield(sr_datasource['id'])\n",
    "df_extra_mappingFields=df_mappingFields.query('is_json_additional_info==True')\n",
    "\n",
    "\n",
    "print(\"==================vIfno mappingFields==================================\")\n",
    "print(df_mappingFields[['column_table_name','field_source_name','is_additional','is_not_null','datasource_id']])\n",
    "print(\"==================vIfno additional mappingFields=========================================\")\n",
    "print(df_extra_mappingFields[['column_table_name','field_source_name','is_additional','is_json_additional_info','datasource_id']])\n",
    "\n",
    "df_vInfo,err=vx.select_colunm_df_fieldname_ds(df_vInfo,df_mappingFields['field_source_name'].tolist(),info_filepath,t_id)\n",
    "list_error.append(err)\n",
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vDisk.info())\n",
    "print(df_vInfo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping filed from datasource to column dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping filed from datasouce to column dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping filed Disk info\")\n",
    "df_vDisk,err=vx.map_fields_ds_to_cols_df(df_vDisk,df_disk_mappingFields,disk_filepath,t_id)\n",
    "list_error.append(err)\n",
    "print(\"Mapping filed VM info\")\n",
    "df_vInfo,err=vx.map_fields_ds_to_cols_df(df_vInfo,df_mappingFields,info_filepath,t_id)\n",
    "list_error.append(err)\n",
    "\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter VM Info and Check Duplicate VM and Remove SystemVM(Reboot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from config database\n",
    "strExVM=\"vCLS-,TestTemp\"\n",
    "exVM=tuple(strExVM.split(','))\n",
    "print(\"Exclude VM : \")\n",
    "df_vInfo=df_vInfo[~df_vInfo.vm.str.startswith((exVM))]\n",
    "print(df_vInfo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filter VM Info and Check Duplicate VM\")\n",
    "\n",
    "\n",
    "isNoteTemplate = False \n",
    "df_vInfo=df_vInfo.query(\"template==@isNoteTemplate\")\n",
    "\n",
    "#vm_state= 'poweredOn'\n",
    "#df_vInfo=df_vInfo.query(\"powerstate==@vm_state  and  template==@isNoteTemplate and primary_ip_address==primary_ip_address\") \n",
    "#df_vInfo=df_vInfo.query(\"template==@isNoteTemplate and primary_ip_address==primary_ip_address\")\n",
    "\n",
    "if len(df_vInfo.index)==0:\n",
    "  list_error.append(True)\n",
    "  error_message= \"no vm data\"\n",
    "  print(error_message)   \n",
    "  vm_util.add_error_to_database(11,error_message,t_id)  \n",
    "                       \n",
    "\n",
    "check_error_point_etl(t_id)\n",
    "\n",
    "df_dup,is_error=vx.find_SameVMName(df_vInfo,'vm')\n",
    "if df_dup is not None:\n",
    "  list_error.append(is_error)\n",
    "  error_message= \"found some same vm name\\n\"\n",
    "  error_message+=df_dup.to_html()  \n",
    "  vm_util.add_error_to_database(13,error_message,t_id)  \n",
    "\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Null Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check Null Value\")\n",
    "\n",
    "#not check veeam-one because data is got rid of all of null veluse to be 0  extract_veeam_report_data zero \n",
    "vdisk_notNaN_cols=(df_disk_mappingFields.query('is_not_null==True')['column_table_name']).tolist()\n",
    "vinfo_notNaN_cols=(df_mappingFields.query('is_not_null==True')['column_table_name']).tolist()\n",
    "\n",
    "print('list all column must be not null')\n",
    "print(vdisk_notNaN_cols)\n",
    "print(vinfo_notNaN_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srDisk_NaN,dfDisk_NaN=vx.find_NaN(df_vDisk,vdisk_notNaN_cols)\n",
    "str_diskNaN_error,diskNaN_error=vx.report_ErrorValues(srDisk_NaN,dfDisk_NaN[['vm_disk']+vdisk_notNaN_cols],t_id,\"Null\",8)\n",
    "list_error.append(diskNaN_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srVM_NaN,dfVM_NaN=vx.find_NaN(df_vInfo,vinfo_notNaN_cols) \n",
    "str_error,x_error=vx.report_ErrorValues(srVM_NaN,dfVM_NaN[['vm']+vinfo_notNaN_cols],t_id,\"Null\",8)\n",
    "list_error.append(x_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Veeam and Disk to VM Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Veeam to VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    " if (is_only_check_data==False) and (use_backup==True):   \n",
    "   print(\"Merge Veeam  to VM Info\") \n",
    "   df_vInfo=df_vInfo.merge(df_veeam[['VM','backup_size_gb']],how='outer',left_on='vm',right_on='VM')\n",
    "   df_vInfo['vm']=df_vInfo.apply(lambda x: x['VM'] if vx.isnan(x['vm']) else x['vm'] ,axis=1)\n",
    "   df_vInfo.drop(columns='VM',inplace=True)\n",
    " else:\n",
    "   df_vInfo['backup_size_gb']=0\n",
    " \n",
    "except Exception as err:\n",
    " list_error.append(True)\n",
    " error_message= str(err)\n",
    " print(error_message)   \n",
    " vm_util.add_error_to_database(14,error_message,t_id) \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter & Transform Disk Info before Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Group disks before merge\")\n",
    "df_vDisk=df_vDisk.query('template_disk==False')\n",
    "print(\"Sum total capacity by vm-id\")\n",
    "df_vDisk=df_vDisk.groupby('vm_disk_id',as_index=False).sum()\n",
    "print(df_vDisk.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge vDisk to VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merg Disk to VM Info\")\n",
    "try:\n",
    " df_vInfo=df_vInfo.merge(df_vDisk[['vm_disk_id','capacity_gb']],how='left',left_on='vm_id',right_on='vm_disk_id')\n",
    " df_vInfo.drop(columns='vm_disk_id',inplace=True)\n",
    "    \n",
    " # replace  numberic joined value with 0 \n",
    " df_vInfo= df_vInfo.replace( {'backup_size_gb':{np.nan:0}, 'capacity_gb':{np.nan:0} })   \n",
    "  \n",
    "except Exception as err:\n",
    " list_error.append(True)\n",
    " error_message= str(err)\n",
    " print(error_message)   \n",
    " vm_util.add_error_to_database(14,error_message,t_id) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data after merge\")\n",
    "print(df_vInfo.info())\n",
    "print(df_vInfo[['vm','vm_id','cpu','backup_size_gb','backup_size_gb','os','database','system_name','cost_center','created_date','terminated_date']].head(10))\n",
    "\n",
    "#df_vInfo.to_excel('vinfo.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill VM data for OnlyBackup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fill VM data for OnlyBackup\")\n",
    "\n",
    "\n",
    "numbericCols = vc.list_master_cost()\n",
    "numColsList = numbericCols['column_table_name'].tolist()\n",
    "nonNumeric_cols = [x for x in df_vInfo.columns.tolist() if x in numColsList]\n",
    "#nonNumeric_cols\n",
    "\n",
    "\n",
    "backup_size_colName='backup_size_gb'\n",
    "notCopyCol=[backup_size_colName]\n",
    "\n",
    "numericColToSetDefaultVal=nonNumeric_cols\n",
    "numericColToSetDefaultVal.remove(backup_size_colName)\n",
    "\n",
    "objectColToSetUnknown=vinfo_notNaN_cols.copy()\n",
    "objectColToSetUnknown=[x for x in  objectColToSetUnknown if  x not in numericColToSetDefaultVal]\n",
    "objectColToSetUnknown.remove('powerstate')\n",
    "objectColToSetUnknown.remove('vm')\n",
    "\n",
    "print(numericColToSetDefaultVal)\n",
    "print(objectColToSetUnknown)\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# create columne only backup first\n",
    "df_vInfo['only_backup']  =False\n",
    "\n",
    "print('No.row of vInfo DF before spliting=',df_vInfo.shape[0])\n",
    "\n",
    "# splite backup datafraom from main dataframe \n",
    "df_none_vm=df_vInfo[df_vInfo['vm_id'].isna()].copy()\n",
    "df_vInfo.drop(df_none_vm.index , inplace=True)\n",
    "\n",
    "print('No.row of vInfo DF after spliting=',df_vInfo.shape[0])\n",
    "print('No.row of noneVM DF after spliting=',df_none_vm.shape[0])\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "if df_none_vm.shape[0]>0:\n",
    "\n",
    " print('These are only backup data') \n",
    " df_none_vm['only_backup']  =True\n",
    " df_none_vm['powerstate']  =unknow_val\n",
    " df_none_vm[numericColToSetDefaultVal]=0.0\n",
    "\n",
    " print(df_none_vm[['vm','vm_id','cpu','memory','capacity_gb',backup_size_colName,'os','database','system_name','cost_center']])\n",
    " \n",
    " list_onlybackup_df=[]\n",
    "\n",
    "\n",
    " tb_sr = vm_util.get_value_by_key(vm_tb_key)\n",
    " if tb_sr is not None:\n",
    "   tb_name = tb_sr['value']\n",
    "\n",
    "   excols_none_vm=df_none_vm.columns.tolist()\n",
    "   #print(excols_none_vm)   \n",
    "   for index, item  in df_none_vm.iterrows(): \n",
    "        \n",
    "     # find the same vm inf as vm-name in backup \n",
    "     #found historical record \n",
    "     sr_prev_vm=vc.get_x_info(tb_name,vm_col_key,item['vm'])\n",
    "     if  sr_prev_vm is not None:\n",
    "     \n",
    "      for blank_col in excols_none_vm:\n",
    "       if (blank_col in  sr_prev_vm.index) and (blank_col in objectColToSetUnknown):\n",
    "        #print('found object : ',excol)\n",
    "        item[blank_col]= sr_prev_vm[blank_col]   \n",
    "        \n",
    "        \n",
    "         \n",
    "     else: #either set defaul value or None  \n",
    "        \n",
    "        for blank_col in excols_none_vm:\n",
    "         if (blank_col not in numericColToSetDefaultVal) and (blank_col in objectColToSetUnknown ) :\n",
    "           item[blank_col]=unknow_val\n",
    "        \n",
    "        item['created_date']=datetime.datetime(1900,1,1)\n",
    "        \n",
    "     # found backup but not found vm , we use init datetime  \n",
    "     list_onlybackup_df.append(item.to_frame().transpose())  \n",
    "   \n",
    " df_none_vm=None\n",
    " df_none_vm=pd.concat(list_onlybackup_df)\n",
    "\n",
    " print(\"VM found backup only after transforming data\")\n",
    " print(df_none_vm[['vm','vm_id','cpu','memory','capacity_gb',backup_size_colName,'system_name','cost_center','created_date']]) \n",
    "   \n",
    "\n",
    "else:\n",
    "   print('No only backup data') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DateTime Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert  datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "lastDay_val=datetime.date(int(year_param),int(month_param),calendar.monthrange(int(year_param), int(month_param))[1])\n",
    "print(lastDay_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate CreatedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validate CreatedDate\")\n",
    "\n",
    "df_vInfo=vx.find_incorrrect_format_for_input_datetime(df_vInfo,\"created_date\",\"vm\",datetime_format,t_id)\n",
    "if df_vInfo is None:\n",
    " list_error.append(True)\n",
    " print(list_error)\n",
    "else:\n",
    " print(df_vInfo[['vm','created_date']] ) \n",
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vInfo=vx.find_invalid_x_date_greater_last_date_for_input_datetime(df_vInfo,\"created_date\",lastDay_val,\"vm\",t_id)\n",
    "if df_vInfo is None:\n",
    " list_error.append(True) \n",
    " print(list_error)\n",
    "else:\n",
    " print(f\"all created_date > {lastDay_val}\")   \n",
    " print(df_vInfo[['vm','created_date']] ) \n",
    "    \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Power On must be empty value in TerminatedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check poweredOn not allow to contain terminated_date.\")\n",
    "print(\"Check poweredOff  allow to not contain terminated_date but costing module will apply last day of month instead.\")\n",
    "\n",
    "# df_off_null = df_vInfo[ (df_vInfo['powerstate']=='poweredOff')  & (df_vInfo['terminated_date'].isnull())].copy()\n",
    "# if  len(df_off_null.index)>0:\n",
    "#    x=df_off_null[['vm','created_date','terminated_date','powerstate']]\n",
    "#    vm_util.add_error_to_database(1,f\"Error because some off-line vm contain null value in terminated_date.<br>{x.to_html(index=False)}\",t_id)\n",
    "#    print(x)\n",
    "#    list_error.append(True)\n",
    "#    print(list_error) \n",
    "    \n",
    "\n",
    "df_on_not_null = df_vInfo[ (df_vInfo['powerstate']=='poweredOn')  & (df_vInfo['terminated_date'].notnull())].copy()\n",
    "if  len(df_on_not_null.index)>0:\n",
    "   x=df_on_not_null[['vm','created_date','terminated_date','powerstate']] \n",
    "   vm_util.add_error_to_database(29,f\"Error because  some  online vm contain value in terminated_date.<br>{x.to_html(index=False)}\",t_id) \n",
    "   print(x)\n",
    "   list_error.append(True)\n",
    "   print(list_error) \n",
    "\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate PowerOff  and TerminatedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Split VM-poweredOff data from main dataflow to ease to process condition\")\n",
    "dfx_off = df_vInfo[ (df_vInfo['powerstate']=='poweredOff')].copy()\n",
    "print(dfx_off[['vm','created_date','terminated_date','powerstate']])\n",
    "\n",
    "df_vmOff_nullTerminatedDate=dfx_off[dfx_off['terminated_date'].isnull() ].copy()\n",
    "df_vmOff_notNullTerminatedDate=dfx_off[dfx_off['terminated_date'].notnull() ].copy()\n",
    "print(df_vmOff_nullTerminatedDate)\n",
    "print(df_vmOff_notNullTerminatedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(df_vmOff_notNullTerminatedDate.index)>0 :  # update 31 Mary 22 on pandas 1.3 but origianl it is 1.1\n",
    "\n",
    "print(f\"VM-poweredOff Convert  terminated_date from string to datetime only row that contain value\")\n",
    "df_vmOff_notNullTerminatedDate=vx.find_incorrrect_format_for_input_datetime(df_vmOff_notNullTerminatedDate,\"terminated_date\",\"vm\",datetime_format,t_id)\n",
    "print(df_vmOff_notNullTerminatedDate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df_vmOff_notNullTerminatedDate is not None and df_vmOff_notNullTerminatedDate.empty==False :\n",
    " print(\"List VM-Off contain terminated_date\")   \n",
    " print(df_vmOff_notNullTerminatedDate[['vm','created_date','terminated_date','powerstate']])\n",
    " print(df_vmOff_notNullTerminatedDate.info())   \n",
    "\n",
    "\n",
    "#  nan_xyz = np.nan\n",
    " print(f\"Check error if  terminated_date >  created_date  or  terminated_date > {lastDay_val}\")\n",
    " #dfx_off_error1=df_vmOff_notNullTerminatedDate.query(\"( terminated_date != @nan_xyz)   and ( created_date>terminated_date or terminated_date>@lastDay_val)\")\n",
    " dfx_off_error1= df_vmOff_notNullTerminatedDate[ df_vmOff_notNullTerminatedDate['terminated_date'] \\\n",
    "                                                < df_vmOff_notNullTerminatedDate['created_date'] ]\n",
    " print(dfx_off_error1)\n",
    "\n",
    " if  len(dfx_off_error1.index)>0:\n",
    "   x=dfx_off_error1[['vm','created_date','terminated_date','powerstate']] \n",
    "   print(x)\n",
    "   vm_util.add_error_to_database(29,f\"found one of the following errors  1.created_date > terminated_date 2.terminated_date > {lastDay_val}. 3.found terminated date of prev month despite existing VM {x.to_html(index=False)}\",t_id) \n",
    "\n",
    "   list_error.append(True)   \n",
    "\n",
    "\n",
    " print(f\"Check error if  terminated_date is not  in month==>{month_param}\")\n",
    "# run month 8  but terminated_date =7 ==> this vm have to be delterd since last month\n",
    "#if existing  , this value must be empty or terminated_date = currunt month\n",
    "\n",
    " dfx_off_error2=df_vmOff_notNullTerminatedDate[ (df_vmOff_notNullTerminatedDate['terminated_date'] < datetime.datetime(int(year_param),int(month_param),1)) | (df_vmOff_notNullTerminatedDate['terminated_date'] >= datetime.datetime(int(year_param),int(month_param)+1,1)) ]\n",
    " print(dfx_off_error2)\n",
    "\n",
    "\n",
    " if dfx_off_error2.shape[0]>0:\n",
    "   y=dfx_off_error2[['vm','created_date','terminated_date','powerstate']] \n",
    "   print(y)\n",
    "   begin_date_str=(datetime.datetime( int(year_param),int(month_param),1)).strftime(\"%Y-%m-%d\") \n",
    "   vm_util.add_error_to_database(29,f\"found one of the following errors 1.created_date > terminated_date 2.terminated_date > {lastDay_val}(terminated_date of the future  month). 3.terminated_date < { begin_date_str}(terminated_date of the past month) {y.to_html(index=False)}\",t_id)  \n",
    "   list_error.append(True)\n",
    "\n",
    "# else:\n",
    "#     print(\"no vm specified terminated date\")\n",
    "          \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge vmOff_nullTerminatedDate with vmOff_notNullTerminatedDate\")\n",
    "dfx_off=pd.concat([df_vmOff_nullTerminatedDate,df_vmOff_notNullTerminatedDate]) \n",
    "dfx_off.reset_index(drop=True,inplace=True)\n",
    "print(dfx_off[['vm','created_date','terminated_date','powerstate']])   \n",
    "print(dfx_off.info())   \n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Split poweredOn data from main dataflow to merge with poweredOff\")\n",
    "dfx_on=df_vInfo[ ~df_vInfo['vm'].isin(dfx_off['vm'].tolist())  ]\n",
    "print(dfx_on[['vm','created_date','terminated_date','powerstate']])\n",
    "\n",
    "\n",
    "print(f\"Merge poweredOn with poweredOff to be complete data for building report\")\n",
    "df_vInfo=pd.concat([dfx_on,dfx_off])\n",
    "df_vInfo.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(df_vInfo.info())\n",
    "print(\"VM Validation has been completed\")\n",
    "print(df_vInfo[['vm','created_date','terminated_date','powerstate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge only backup to vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_none_vm.shape[0]>0:\n",
    " df_vInfo=pd.concat([df_vInfo,df_none_vm],ignore_index=True,axis=0)\n",
    "\n",
    "check_error_point_etl(t_id)\n",
    "\n",
    "print(df_vInfo.head()) \n",
    "print(df_vInfo.tail()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Non-Numberic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Non-Numberic Data\n",
    "print(\"Find Non-Numberic (Do after only back)\")\n",
    "srNN, dfNN,colsNN= vx.find_NonNumeric(df_vInfo)\n",
    "print(colsNN)\n",
    "print(\"the following columns are required numeric value\" ,colsNN)\n",
    "str_error, error=vx.report_ErrorValues(srNN,dfNN[['vm']+colsNN],t_id,\"Non-Numeric\",15)\n",
    "#print(srNN)\n",
    "#print(dfNN)\n",
    "\n",
    "list_error.append(error)\n",
    "print(str_error)\n",
    "print(error)\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vInfo.to_excel('vinfo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vInfo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform VM Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filter and Transform VM Info\")\n",
    "\n",
    "try:\n",
    "\n",
    "    \n",
    " df_vInfo['month']=month_param\n",
    " df_vInfo['year']=year_param\n",
    " df_vInfo['import_date']=x_datenow\n",
    "\n",
    "\n",
    " df_vInfo['memory']=df_vInfo['memory'].apply(vm_util.set_size_gb)\n",
    "\n",
    "\n",
    " df_vInfo['capacity_gb']=df_vInfo['capacity_gb'].apply(vm_util.set_size_gb)\n",
    "\n",
    " df_vInfo['database_no_cal'] = df_vInfo['database_no_cal'].apply(pd.to_numeric, errors='coerce')     \n",
    " df_vInfo= df_vInfo.replace( {'database_no_cal':{np.nan:0}}) \n",
    "\n",
    "# add transaction ID\n",
    " df_vInfo['transaction_id']= t_id\n",
    "    \n",
    " # replace nan with None then call replace_x_character  \n",
    "\n",
    "\n",
    " df_vInfo=df_vInfo.where(pd.notnull(df_vInfo), None)\n",
    "\n",
    " \n",
    "    \n",
    " df_vInfo['system_name']=df_vInfo['system_name'].apply(vx.replace_x_character)\n",
    " df_vInfo['cost_center']=df_vInfo['cost_center'].apply(vx.replace_x_character)   \n",
    " df_vInfo['database']=df_vInfo['database'].apply(vx.replace_x_character) \n",
    " \n",
    "\n",
    "except Exception as err:\n",
    "  list_error.append(True)\n",
    "  error_message= str(err)\n",
    "  print(error_message)   \n",
    "  vm_util.add_error_to_database(14,error_message,t_id)  \n",
    "  \n",
    "\n",
    "    \n",
    "check_error_point_etl(t_id)\n",
    "\n",
    "\n",
    "\n",
    "vinfo_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VInfo data completed\")\n",
    "print(df_vInfo.info())\n",
    "print(df_vInfo[['vm','powerstate','created_date','terminated_date','import_date','month','year']])\n",
    "\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End up with  Completed ETL in only Checking Data mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_only_check_data==True:\n",
    "    try: \n",
    "        for file in  [disk_filepath ,info_filepath]:\n",
    "         delete_resultList= fd_mn.delele_file(file)   \n",
    "        \n",
    "        updated_rows=vm_util.created_transaction(t_id)\n",
    "        print(\"completed ETL in only Checking Data mode\")\n",
    "        \n",
    "        exit()\n",
    "        \n",
    "        \n",
    "    except Exception as ex:\n",
    "        list_error.append(True)\n",
    "        print(ex)\n",
    "        raise Exception(\"Program is teminated and check error from email and log_error.txt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create additional dynamic field/column by json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create active additiona dynamic field/column by json\")\n",
    "extraFieldList=df_extra_mappingFields['column_table_name'].tolist()\n",
    "print(extraFieldList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_additional_field_json(row):\n",
    "    dict_fileds={}\n",
    "    \n",
    "    for key in extraFieldList: \n",
    "        valx=row[key]\n",
    "        nan_valx=pd.isnull(valx)\n",
    "        if  (nan_valx==False):\n",
    "            dict_fileds[key]=valx\n",
    "            \n",
    "#     if len(dict_fileds.keys())==0 :\n",
    "#      #df.replace('N/A',np.NaN)\n",
    "#      return None\n",
    "#      # return {}\n",
    "#     else   : \n",
    "    return json.dumps(dict_fileds)\n",
    "\n",
    "   \n",
    "df_vInfo['additional_info']=df_vInfo.apply(add_additional_field_json,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vInfo.head())\n",
    "print(df_vInfo.info())\n",
    "#df_vInfo.to_excel('vm.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add datafrome to database\")\n",
    "#get table name from key value\n",
    "table_name,listCols_report_vInfo,is_error=vx.listCols_report_table(vm_tb_key,t_id)\n",
    "list_error.append(is_error)\n",
    "\n",
    "check_error_point_etl(t_id)\n",
    "print(f'list all columns in {table_name} table: ',len(listCols_report_vInfo))\n",
    "print(listCols_report_vInfo)\n",
    "\n",
    "# test_remove={}\n",
    "# mostwanted_col = [ele for ele in dfCols_report_vInfo[column_name]\n",
    "#                   if ele not in test_remove] \n",
    "# print('mostwanted_col:',mostwanted_col)\n",
    "# listCols_report_vInfo=mostwanted_col\n",
    "\n",
    "print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "listCols_dfx=df_vInfo.columns.tolist()\n",
    "print(f'list all columns in {table_name} dataframe: ',len(listCols_dfx))\n",
    "print(listCols_dfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_columns_2=vm_util.verify_existing_columns(listCols_dfx,listCols_report_vInfo)\n",
    "\n",
    "print(check_columns_2)\n",
    "listCols_report_vInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_columns_2  is None:\n",
    "   df_vInfo=df_vInfo[listCols_report_vInfo]\n",
    "   print(df_vInfo.info())\n",
    "   #df_vInfo.to_excel('report_vInfo_before_DB.xlsx')\n",
    "   print(df_vInfo.head())\n",
    "   rslt=db_command.add_data_values(db_command.get_postgres_conn(),df_vInfo,table_name)\n",
    "   #print(rslt)\n",
    "    \n",
    "else:\n",
    "   list_error.append(True)\n",
    "   error_message=check_columns_2\n",
    "   vm_util.add_error_to_database(10,error_message,t_id)\n",
    "   print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update new cost center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Update new cost center\")\n",
    "\n",
    "try :\n",
    " cost_centerList = df_vInfo['cost_center'].unique()\n",
    " df_new_cc=vm_util.add_new_cost_center(cost_centerList,'VM-ETL',t_id)\n",
    " print(df_new_cc)   \n",
    "except Exception as ex:\n",
    " list_error.append(True)   \n",
    " print(ex)   \n",
    "\n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upate new addtional cost(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upate new addtional cost(category)\")\n",
    "df_vInfo=vm_util.create_extra_fields_byJson(df_vInfo)\n",
    "df_vInfo=df_vInfo.where(pd.notnull(df_vInfo), None)\n",
    "\n",
    "print(\"List addtional cost type for update\")\n",
    "print(df_vInfo.info())\n",
    "\n",
    "#df_vInfo.to_excel(\"report_vInfo_afterAddCate_DB.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cost=[]\n",
    "def collect_new_addtional_to_sent_mail(df_new_cost,tt_id):\n",
    "#'CostID', 'CostName','CateID','CateName'  \n",
    " try:\n",
    "     link_update_price=vm_util.get_config_value(update_additional_cost_key, tt_id)\n",
    "     link_all_price=vm_util.get_config_value(all_additional_cost_key, tt_id)   \n",
    "     content_data_dict= {\n",
    "                     \"ContentTitle\":\"List New Addional Cost\",\n",
    "                     \"New_Additional_Cost\":df_new_cost,\n",
    "                     \"Len_Cols_New_Additional_Cost\": len(df_new_cost.columns),\n",
    "                     'LinkAll':link_all_price,\n",
    "                     'LinkEach':link_update_price\n",
    "                    } \n",
    "     ok= x_mail.send_email(email_type='new_additional_cost',transaction_id=tt_id,\n",
    "                     attached_file_path=None,content_data_dict=content_data_dict)  \n",
    "     return ok   \n",
    "    \n",
    " except Exception as ex:\n",
    "     print(str(ex))   \n",
    "     list_error.append(True)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_category_cost(cost_name,cate_column_id,cate_column_name):\n",
    " # sql = \"select cost_name from master_additional_cost  where  id =1\"\n",
    " \n",
    " cost_id =0;\n",
    " try  : \n",
    "        \n",
    "    if (cost_name is not None) and (cost_name !=unknow_val) :\n",
    "     cost_name=str(cost_name)   \n",
    "     #print('xyz: ',cate_column_id,'-',cost_name)\n",
    "     x=cost_name.replace(' ','').lower()\n",
    "     sql=\"select cost_name from master_additional_cost  where cost_column_id = (%s) and lower(replace(cost_name, ' ', '')) = (%s)\"\n",
    "     params=(cate_column_id,x)\n",
    "     cost_item=db_command.get_one_sql(db_command.get_postgres_conn(),sql,params)\n",
    "     #print('type-x',type(cost_item))\n",
    "     \n",
    "     if(cost_item is None):\n",
    "       #print('new-item : ',cost_item,' as ',type(cost_item))     \n",
    "       params = (cate_column_id, cost_name,cost_name)\n",
    "       sql_insert=\"\"\"INSERT INTO master_additional_cost(cost_column_id,cost_name,description)\n",
    "             VALUES(%s,%s,%s) RETURNING id;\"\"\"\n",
    "       cost_id=db_command.add_one_data_sql(db_command.get_postgres_conn(),sql_insert,params)\n",
    "       new_cost.append([cost_id,cost_name,cate_column_name,cost_name]) \n",
    "    \n",
    " except Exception as err:\n",
    "    list_error.append(True)\n",
    "    error_message= str(err)\n",
    "    print(error_message)   \n",
    "    vm_util.add_error_to_database(12,error_message,t_id)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sql=\"select * from datafield_mapping where id in( select distinct on (cost_column_id) cost_column_id from master_additional_cost UNION SELECT id from datafield_mapping where is_additional=True and is_active=True)\"\n",
    "\n",
    "    print(\"List category cost for update data\")\n",
    "    list_cate_cost=db_command.get_list_sql(db_command.get_postgres_conn(),sql,None)\n",
    "    df_cateCost=None\n",
    "    if list_cate_cost is  not None:\n",
    "     df_cateCost=pd.DataFrame(list_cate_cost)\n",
    "     print(df_cateCost)  \n",
    "\n",
    "except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    print(str(ex))   \n",
    "    vm_util.add_error_to_database(12,str(ex),t_id)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_category_cost(cate_column_id,cost_name):\n",
    "\n",
    "if df_cateCost is not None :\n",
    " for index,row  in  df_cateCost.iterrows():\n",
    "      cate_column_id=row['id']\n",
    "      costtype_name=row['column_table_name']\n",
    "      if costtype_name in listCols_dfx: # update only additional type in df columns \n",
    "         print(\"=============================================\")\n",
    "         #print(cate_column_id,'-',costtype_name)\n",
    "         #print(df_vInfo[costtype_name])\n",
    "         df_vInfo[costtype_name].apply(new_category_cost,args=(cate_column_id,costtype_name))\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get new additonal cost (cost type and cost anme ) in order to notify admin for adding price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_cost)>0:\n",
    " df_new_additonal_cost = pd.DataFrame(new_cost, columns = ['CostID', 'CostName','CateName','description'])\n",
    " print(df_new_additonal_cost) \n",
    " collect_new_addtional_to_sent_mail(df_new_additonal_cost,t_id)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completed ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veeam_ok=True  veeam_filepath\n",
    "# vdisk_ok=True  disk_filepath\n",
    "# vinfo_ok=True  info_filepath\n",
    "#happy case\n",
    "try: \n",
    "    #etl_files=[[veeam_ok,veeam_filepath],[vdisk_ok,disk_filepath] ,[vinfo_ok,info_filepath]]\n",
    "    \n",
    "    # disable veeam\n",
    "    etl_files=[[vdisk_ok,disk_filepath] ,[vinfo_ok,info_filepath]]\n",
    "    \n",
    "    if veeam_ok==True and vdisk_ok==True  and vinfo_ok==True:\n",
    "     vm_util.finished_etl_folder(t_id,vm_col_key,True,etl_files)    \n",
    "     updated_rows=vm_util.created_transaction(t_id)\n",
    "     print(\"completed ETL\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    print(ex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
