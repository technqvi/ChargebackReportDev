{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chargeback_rpt.db_postgres_command as db_command\n",
    "import chargeback_rpt.vm_data_utility as vm_util\n",
    "\n",
    "import chargeback_rpt.vm_data_validator as vx\n",
    "import chargeback_rpt.file_directory_manager as fd_mn\n",
    "import chargeback_rpt.email_notifier as x_mail\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import hpestorapi;\n",
    "\n",
    "import random\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ip = '0.0.0.0'\n",
    "x_username = 'abc'\n",
    "x_password = 'xyz'\n",
    "\n",
    "#https://stackoverflow.com/questions/5194057/better-way-to-convert-file-sizes-in-python\n",
    "x_convert_unit=30  # convert bytes to gb\n",
    "x_unit_str='GB'\n",
    "#x_convert_unit=40  # convert bytes to tb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name='name'\n",
    "col_createdDate='createdDate'\n",
    "col_userBytes='userBytes'\n",
    "col_diskBytes='diskBytes'\n",
    "\n",
    "\n",
    "tb_key=\"storeonce_table\"\n",
    "# catalyst_name  user_size   disk_size  reated_date month  year import_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_py=False\n",
    "fix_mmyy=False\n",
    "is_only_check_data=False\n",
    "\n",
    "x_datenow=datetime.datetime.now()\n",
    "month_x=x_datenow.strftime('%m')\n",
    "year_x=x_datenow.strftime('%Y')\n",
    "\n",
    "if fix_mmyy:\n",
    " month_param='09'\n",
    " year_param='2022'\n",
    "else:\n",
    " month_param=month_x\n",
    " year_param=year_x\n",
    "    \n",
    "list_error=[]\n",
    "print(list_error)\n",
    "\n",
    "print(f\"{month_param}-{year_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  run_py:\n",
    "\n",
    "    press_y='n'\n",
    "    ok=False\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        if sys.argv[1]=='0'  and sys.argv[2]=='0':\n",
    "         month_param=month_x\n",
    "         year_param=year_x \n",
    "         ok=True   \n",
    "         is_only_check_data=bool(int(sys.argv[3]))\n",
    "        else:\n",
    "         raise Exception(\"Specify 0 0 as arguments only\")   \n",
    "    else:\n",
    "        print(\"If you want current month and year.Press Enter \")\n",
    "        month_param = int(input(\"Enter the month (1-12) : \") or month_x)\n",
    "        year_param = int(input(\"Enter the year(such as 2021,2022,2023...) : \") or year_x)\n",
    "        print(f\"Do for month={month_param} and year={year_param}\")\n",
    "        press_y=input(f\"Press y=True and n=False : \") \n",
    "        if press_y.lower()=='y':\n",
    "         ok=True\n",
    "\n",
    "\n",
    "    if ok==True:\n",
    "        try:\n",
    "         report_dt= datetime.datetime(int(year_param), int(month_param), 1)\n",
    "         print(report_dt)   \n",
    "         month_param=report_dt.strftime('%m')\n",
    "         year_param=report_dt.strftime('%Y')   \n",
    "\n",
    "\n",
    "        except Exception as ex:\n",
    "         print(ex)   \n",
    "         raise ex\n",
    "    else:\n",
    "         quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move  to util\n",
    "def check_error_point_etl(tran_id):\n",
    "  print(list_error)\n",
    "  if True in list_error:\n",
    "        \n",
    "    vm_util.collect_error_to_sent_mail(tran_id) \n",
    "    \n",
    "    print(\"ETL occured error\") \n",
    "    \n",
    "    raise Exception(\"Program is teminated and check error from email and log_error.txt\")\n",
    "    \n",
    "  list_error.clear()\n",
    "  print(list_error)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_id=vm_util.creating_transaction(13,month_param,year_param)\n",
    "print(f\"ETL Transaction ID: {t_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeonce_prefex_site_name=vm_util.get_config_value('storeonce_prefex_site_name',t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get  data from StoreOnce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"get  data from StoreOnce and enrich data to dataframe\")\n",
    "def getCatalystInfo():\n",
    "    \n",
    " try:  \n",
    "    with hpestorapi.StoreOnceG4(x_ip, x_username, x_password) as so:\n",
    "        so.open()\n",
    "        status, data = so.get('/api/v1/data-services/cat/stores')\n",
    "        if status == 200:\n",
    "         \n",
    "         df_x=None\n",
    "         list_catalystName=[]   \n",
    "         list_createdDate=[]\n",
    "         list_userSize=[]\n",
    "         list_diskSize=[]\n",
    "        \n",
    "#          for x in ['BPK-Catalyst'] :        \n",
    "                \n",
    "         for group in data['members']:\n",
    "\n",
    "            catalystName=group['name']\n",
    "            catalystName=catalystName.replace(storeonce_prefex_site_name,'')\n",
    "            createdDate=group['createdDate']              #convert date as yyyy-mm-ddd format\n",
    "            userBytes=group['userBytes']\n",
    "            userSize=float(userBytes)/float(1<<x_convert_unit)\n",
    "            diskBytes=group['diskBytes']\n",
    "            diskSize=float(diskBytes)/float(1<<x_convert_unit)\n",
    "\n",
    "            print(f'{catalystName} - {createdDate}')\n",
    "            print(f'Total Disk (userBytes) ={userBytes} Bytes  =  {userSize} {x_unit_str}')\n",
    "            print(f'ActualUsage Disk (diskBytes) ={diskBytes} Bytes  =  {diskSize} {x_unit_str}')\n",
    "            print(\"=========================================================\")\n",
    "            \n",
    "            # Test Smaple Data\n",
    "#             catalystName=x.replace(storeonce_prefex_site_name,'')\n",
    "#             createdDate=datetime.datetime.now()\n",
    "#             userSize=random.randint(1000, 10000)\n",
    "#             diskSize=round(userSize/2)\n",
    "    \n",
    "            list_catalystName.append(catalystName)   \n",
    "            list_createdDate.append(createdDate)\n",
    "            list_userSize.append(userSize)   \n",
    "            list_diskSize.append(diskSize)      \n",
    "                  \n",
    "            if  len(list_catalystName)>0:\n",
    "               df_x=pd.DataFrame(data={'catalyst_name':list_catalystName,'created_date':list_createdDate,'user_size':list_userSize,'disk_size':list_diskSize})\n",
    "            \n",
    "            # Incorperate Addtional Data\n",
    "            df_x['month']=month_param\n",
    "            df_x['year']=year_param\n",
    "            df_x['import_date']=x_datenow\n",
    "            \n",
    "         return df_x  \n",
    "   \n",
    " except Exception as error:\n",
    "        error_message=f'Cannot connect to {x_ip} because {str(error)}'\n",
    "        list_error.append(True)\n",
    "        print(error_message)   \n",
    "        vm_util.add_error_to_database(14,error_message,t_id)  \n",
    "\n",
    "            \n",
    "              \n",
    "# 'sizeOnDiskQuotaBytes': 214748364800,\n",
    "# 'userDataStoredQuotaBytes': 4398046511104,\n",
    "# 'userBytes': 5856889966,\n",
    "# 'diskBytes': 3592012364,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalyst=getCatalystInfo()\n",
    "check_error_point_etl(t_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_catalyst.info())\n",
    "df_catalyst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"save  dataframe to database\")\n",
    "table_name,listCols_report,is_error=vx.listCols_report_table(tb_key,t_id)\n",
    "list_error.append(is_error)\n",
    "\n",
    "\n",
    "print(f'list all columns in {table_name} table: ',len(listCols_report))\n",
    "print(listCols_report)\n",
    "\n",
    "listCols_dfx=df_catalyst.columns.tolist()\n",
    "print(f'list all columns in {table_name} dataframe: ',len(listCols_dfx))\n",
    "print(listCols_dfx)\n",
    "\n",
    "\n",
    "check_columns_2=vm_util.verify_existing_columns(listCols_dfx,listCols_report)\n",
    "\n",
    "print(check_columns_2)\n",
    "\n",
    "\n",
    "print(\"Save data to database\")\n",
    "if check_columns_2  is None:\n",
    "   df_catalyst=df_catalyst[listCols_report]\n",
    "   print(df_catalyst.info())\n",
    "   print(df_catalyst.head())\n",
    "   rslt=db_command.add_data_values(db_command.get_postgres_conn(),df_catalyst,table_name)\n",
    "   print(rslt)\n",
    "    \n",
    "else:\n",
    "   list_error.append(True)\n",
    "   error_message=check_columns_2\n",
    "   vm_util.add_error_to_database(10,error_message,t_id)\n",
    "   print(error_message)\n",
    "    \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check New Site For StoreOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_site(siteList,tran_id):\n",
    "   \n",
    "   df_new_site = None\n",
    "   try:\n",
    "\n",
    "        all_cost_center_key = 'email_link_all_site_storeonce'\n",
    "        update_cost_center_key = 'email_link_update_site_storeonce'\n",
    "\n",
    "        new_ccList = []\n",
    "\n",
    "        def insert_new_site(cc_name):\n",
    "            try:\n",
    "                cc_name_low = cc_name.replace(' ', '').lower()\n",
    "                sql = \"select site_name from site_info where lower(replace(site_name, ' ', '')) = (%s) \"\n",
    "                param_x = (cc_name_low,)\n",
    "                cc_item = db_command.get_one_sql(db_command.get_postgres_conn(), sql, param_x)\n",
    "\n",
    "                if cc_item is None:\n",
    "                    print('new site : ', cc_name)\n",
    "                    params = (cc_name, '-', '-', '-','-','-','-')\n",
    "                    sql_insert = \"\"\"\n",
    "                    INSERT INTO site_info(site_name,site_full_name,site_address,site_owner,site_owner_email,site_approver_name,site_approver_position) \n",
    "                    VALUES(%s,%s,%s,%s,%s,%s,%s) RETURNING id;\n",
    "                    \"\"\"\n",
    "                    new_cc_id = db_command.add_one_data_sql(db_command.get_postgres_conn(), sql_insert, params)\n",
    "                    \n",
    "                    new_ccList.append([new_cc_id, cc_name])\n",
    "\n",
    "            except Exception as ex:\n",
    "                error_message = str(ex)\n",
    "                print(error_message)\n",
    "                add_error_to_database(12, error_message, tran_id)\n",
    "                raise ex\n",
    "\n",
    "        def collect_new_costcenter_to_sent_mail():\n",
    "            #mail_new_site_template.html\n",
    "            try:\n",
    "                link_all_cc = vm_util.get_config_value(all_cost_center_key, tran_id)\n",
    "                link_update_cc = vm_util.get_config_value(update_cost_center_key, tran_id)\n",
    "\n",
    "                content_data_dict = {\n",
    "                    \"ContentTitle\": f\"List New Site For StoreOnce\",\n",
    "                    \"New_Site_StoreOnce\": df_new_site,\n",
    "                    \"Len_Cols_New_Site_StoreOnce\": len(df_new_site.columns),\n",
    "                    'LinkAll': link_all_cc,\n",
    "                    'LinkEach': link_update_cc\n",
    "                }\n",
    "                ok = x_mail.send_email(email_type='new_site_catalyst', transaction_id=tran_id,\n",
    "                                       attached_file_path=None, content_data_dict=content_data_dict)\n",
    "#                 ok=True\n",
    "                return ok\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(str(ex))\n",
    "                raise ex\n",
    "\n",
    "\n",
    "        for item in siteList:\n",
    "            insert_new_site(item)\n",
    "\n",
    "        if len(new_ccList) > 0:\n",
    "            df_new_site = pd.DataFrame(new_ccList, columns=['ID', 'SiteName'])\n",
    "            #collect_new_costcenter_to_sent_mail()\n",
    "             \n",
    "   except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "\n",
    "   return df_new_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Update new site\")\n",
    "\n",
    "try :\n",
    " siteList = df_catalyst['catalyst_name'].unique()\n",
    " df_new_cc=add_new_site(siteList,t_id)\n",
    "except Exception as ex:\n",
    " list_error.append(True)   \n",
    " print(ex)   \n",
    "\n",
    "check_error_point_etl(t_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "  \n",
    "     updated_rows=vm_util.created_transaction(t_id)\n",
    "     print(\"completed ETL\")\n",
    "    \n",
    "except Exception as ex:\n",
    "    list_error.append(True)\n",
    "    print(ex)\n",
    "    \n",
    "check_error_point_etl(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
